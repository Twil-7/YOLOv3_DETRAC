1、UA-DETRAC车辆检测数据集： 下载博客 ： 

https://blog.csdn.net/weixin_43653815/article/details/95514857?utm_term=detrac%E6%95%B0%E6%8D%AE%E9%9B%86&utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~all~sobaiduweb~default-0-95514857&spm=3001.4430

ua-detrac数据集上多目标跟踪算法JDE的训练,在,UADETRAC：

https://www.pythonf.cn/read/111858



2、如何解析如此复杂的xml文件呢？？？

UA-DETRAC原始数据集（所有图片分辨率为960x540），标签是XML格式，且一个xml对应一个视频序列，每一个xml内容包含该视频序列中所有帧的标注信息。

我似乎已经看懂了这个xml文件是如何标注信息的。这个文件夹下共有664张图片，对应着视频共有664帧。在这个xml文件下，
<frame density="7" num="1">记录着第1帧图片的标注信息，这张图片中出现了七辆车；<frame density="7" num="2">记录着第2帧的标注信息，这张图片中也出现了七辆车，......，<frame density="6" num="6">记录着第6帧的信息，这张图片中出现的只有六辆车，这样依次不断记录下去，直到<frame density="10" num="664">，第664张图片中记录了10辆车。


其中一帧中包括多个车辆标注，标注信息包括：车辆ID，box坐标，以及一些属性：方向，速度，轨迹长度，遮挡率，车辆类别。



3、找到了一份ua-detrac数据集xml解析源码，希望有用 ！！！ 

https://blog.csdn.net/w5688414/article/details/78931910

.zfill(5)代表的是，把664帧的图像分别改写成5位的格式。

print(child.tag, child.attrib["num"], child.attrib["density"])，依次取出了<frame density="7" num="1">的属性，对应的帧序号，该帧下目标物体数目。

完美解决了！bingo！，其实只要不断利用for循环，然后取出对应的attrib["id"]，就很容易实现xml树的解析。


4、服了，我在编码标注的时候，出现了数据集错位的情况，后面标记混乱的一团糟...

逐个对比，我找到了bug，在第14个文件夹下，由于车辆过于狭小，xml中完全就没有标记对应的bbox，从而后面完全乱序。

我打算重构数据读取思路，不按照索引相互对应，而是只处理xml文件，在有标注的里面再读进去相应的图片，这样就绝对不会出现乱位问题。

# train_x num :  83791      test_x num :  56340
# train_y num :  82085      test_y num :  56167



5、k-means聚类时我人傻了，居然聚类出现这种数值：

[[-9223372036854775808 -9223372036854775808]
 [                   4                   13]
 [                   6                   17]
 [                   8                   23]
 [                  10                   30]
 [                  13                   40]
 [                  19                   53]
 [                  26                   79]
 [                  28                  127]]

-9223372036854775808我真的人傻了，百度查了一下，在Numpy中numpy.nan显示成-9223372036854775808。也就说说，我调用k-means函数聚类时，有一个设置为nan时，才可以聚类出合适结果。


6、总共包含15万张图片，小电脑里面运行完全会被kill，必须要上传到中才能处理。目前我需要做的事情就是，把我这个代码传到git里面，然后再在容器里面拉下来这份代码。


7、如果直接对原始图片数据集进行resize，效果会非常差，因为目标物体被缩放得特别小，连人眼都难以分辨，更别说计算机了。

训练过程中的loss下降艰难，val loss到了70就开始瓶颈了，但惊喜的是，就算是这样，demo的效果特还可以，能检测出来不少车辆。

我发现，所以以后需要进行YOLOv3目标检测时，我都要提前对原始图片做一个变换，剔除掉无关区域再resize，这一步起码能将检测精度提升20%！！！


