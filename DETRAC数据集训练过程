训练了两天多，被迫暂停一会儿。optimizer=Adam(lr=1e-3)。

Epoch 1/10
2565/2565 [==============================] - 86006s 34s/step - loss: 154.7657 - val_loss: 73.8884
Epoch 2/10
2565/2565 [==============================] - 99136s 39s/step - loss: 30.2837 - val_loss: 60.6485
Epoch 3/10
1007/2565 [==========>...................] - ETA: 18:08:52 - loss: 25.8703


2、接着又训练了两个epoch，optimizer=Adam(lr=1e-3)。

Epoch 1/3
2565/2565 [==============================] - 118117s 46s/step - loss: 26.0896 - val_loss: 65.0846
Epoch 2/3
2565/2565 [==============================] - 118138s 46s/step - loss: 24.5482 - val_loss: 63.6707
Epoch 3/3
 372/2565 [===>..........................] - ETA: 25:25:31 - loss: 24.4052
 
 
 
感触：

（1）数据集大小在整个YOLOv3模型训练过程中太过重要，在将整个网络层解冻后，如此深度的网络模型，必须得喂入10w以上的图片数据集，单纯几千-一万的数据量已经无法达到预期效果。

（2）伴随着将整个网络层解冻后，检测可信度的精度问题也同时被解决。针对真实目标物体，网络给出的bounding box精度都有0.8以上，甚至0.99。

（3）虽然这个数据集看上去质量不佳，小目标过多，而且resize后尺寸变形严重。我还没来得及做图片裁减放缩预处理，但在超大数据量下，居然效果依旧很好，令人惊艳。

（4）从这次的训练中我学到了很有用的一点：

以后在搜集车牌号数据集时没必要如此麻烦，只需要拍上百个视频即可，每个视频含有1000帧图片，这样一下数据集就达到十万张。这样数据搜集起来方便了太多，只需要细细标注即可。

一个视频能抵得上成百上千张图片，而且这种视频场景本就是算法最终应用场景，最终测试效果会更佳。
